{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center\">机器学习导论习题六</h1>\n",
    "\n",
    "<h2 style=\"text-align: center;\">学号, 姓名, <a href=\"mailto:TODO@smail.nju.edu.cn\">邮箱</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于记录每个单元格的运行时间\n",
    "\n",
    "try:\n",
    "    %load_ext autotime\n",
    "except:\n",
    "    !pip install ipython-autotime\n",
    "    %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入第三方库\n",
    "\n",
    "import os, re, glob, time, random, datetime\n",
    "import multiprocessing as mp\n",
    "\n",
    "GLOBAL_START_TIME = time.time()\n",
    "\n",
    "# # !pip install ipywidgets widgetsnbextension pandas-profiling\n",
    "# from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "\n",
    "import mindspore\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 128\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固定随机数种子\n",
    "\n",
    "GLOBAL_SEED = 0\n",
    "\n",
    "def fix_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if 'mindspore' in globals():\n",
    "        mindspore.set_seed(seed)\n",
    "\n",
    "fix_seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 [20pts] 处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) [5pts] 加载数据\n",
    "\n",
    "从 `./data/` 中加载数据, 特征数据加载为 `np.float64`, 标签数据加载为 `np.int64`. 注意, 原始数据的标签从 `1` 开始, 你需要转换成从 `0` 开始."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "\n",
    "# 提示: np.loadtxt\n",
    "train_x = None\n",
    "train_y = None\n",
    "test_x = None\n",
    "test_y = None\n",
    "\n",
    "print(f'train_x.dtype={train_x.dtype}; train_x.shape={train_x.shape}; train_y.dtype={train_y.dtype}; train_y.shape={train_y.shape};')\n",
    "print(f'test_x.dtype={test_x.dtype}; test_x.shape={test_x.shape}; test_y.dtype={test_y.dtype}; test_y.shape={test_y.shape};')\n",
    "\n",
    "assert all((train_x.shape == (7352, 561), train_y.shape == (7352,), test_x.shape == (2947, 561), test_y.shape == (2947,)))\n",
    "assert all((train_x.dtype == np.float64, train_y.dtype == np.int64, test_x.dtype == np.float64, test_y.dtype == np.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) [5pts] 检查数据\n",
    "\n",
    "分析并回答如下问题:\n",
    "\n",
    "- 数据中是否存在缺失值?\n",
    "\n",
    "> ⌨在这里回答\n",
    "\n",
    "- 数据是否存在类别不平衡的问题?\n",
    "\n",
    "> ⌨在这里回答\n",
    "\n",
    "- 数据属性取值是否需要归一化?\n",
    "\n",
    "> ⌨在这里回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印训练数据缺失值的统计结果\n",
    "\n",
    "# 提示: np.isnan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印训练数据类别样例数量的统计结果\n",
    "\n",
    "# 提示: np.bincount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印训练数据属性取值的统计结果\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) [5pts] 可视化属性分布\n",
    "\n",
    "属性取值归一化之后, 选择方差最大的特征, 绘制小提琴图, 可视化对比各个类别的样本在该属性上取值分布.\n",
    "\n",
    "绘图参考下图:\n",
    "\n",
    "<div><img src=\"./plot/violinplot.png\" width=\"512\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 挑选方差最大的属性\n",
    "\n",
    "# 提示: np.argmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制小提琴图可视化各个类别在该属性上取值分布\n",
    "\n",
    "# 提示: sns.violinplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) [5pts] 可视化属性相关性\n",
    "\n",
    "绘制热力图, 可视化前 51 个属性两两之间的 Pearson 相关系数.\n",
    "\n",
    "绘图参考下图:\n",
    "\n",
    "<div><img src=\"./plot/heatmap.png\" width=\"512\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制热力图可视化前51个属性两两之间的Pearson相关系数\n",
    "\n",
    "# 提示: sns.heatmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 [15pts+附加5pts] 分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) [5pts] 调用 `sklearn` 实现基线模型\n",
    "\n",
    "固定超参数, 汇报如下基线模型的运行时间和准确率: $k$ 近邻, 高斯核支持向量机 (高斯核又称径向基核), 随机森林.\n",
    "\n",
    "> - $k$ 近邻: elapsed=????s; accuracy=????%; \n",
    "> - 高斯核支持向量机: elapsed=????s; accuracy=????%;\n",
    "> - 随机森林: elapsed=????s; accuracy=????%;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 注意固定随机数种子确保实验可复现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) [5pts] 调用 `xgboost` 实现 Boosting 模型\n",
    "\n",
    "固定超参数, 汇报 `xgboost` 的运行时间和准确率.\n",
    "\n",
    "> - `xgboost`: elapsed=????s; accuracy=????%;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 注意固定随机数种子确保实验可复现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) [5pts] 基于 `torch` 训练神经网络模型\n",
    "\n",
    "每遍历一轮训练数据, 就在 `./ckpt/` 中保存当前模型权重, 固定超参数, 绘制神经网络在训练集和测试集上的准确率随训练轮数变化的折线图.\n",
    "\n",
    "绘图参考下图:\n",
    "\n",
    "<div><img src=\"./plot/line.png\" width=\"512\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: list, output_dim: int):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.hidden_layers = [\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim_in, dim_out),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            for dim_in, dim_out in zip(hidden_dims[:-1], hidden_dims[+1:])\n",
    "        ]\n",
    "        self.hidden_layers = nn.Sequential(*self.hidden_layers)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[-1], output_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = hidden_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "class MLPClassifier(object):\n",
    "    def __init__(self, hidden_dims: list = [128, 32], batch_size: int = 128, learning_rate: float = 1e-2, num_epochs: int = 16, ckpt_dir: str = None):\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "    def define_mlp(self, x: np.ndarray, y: np.ndarray):\n",
    "        _, self.input_dim = x.shape\n",
    "        self.output_dim = y.max() + 1\n",
    "        self.mlp = MLP(input_dim=self.input_dim, hidden_dims=self.hidden_dims, output_dim=self.output_dim)\n",
    "    def train_mlp(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.mlp.train()\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(y).long()\n",
    "        dataset = TensorDataset(x, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.mlp.parameters(), lr=self.learning_rate)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in dataloader:\n",
    "                raise NotImplementedError()\n",
    "                # TODO: 阅读官方文档示例, 完成梯度下降的代码, 注意把当前批次的损失加到 running_loss 上.\n",
    "                # https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop\n",
    "            print(f'\\033[1m[{epoch+1:3d}/{self.num_epochs:3d}]\\033[0m running_loss={running_loss:8.4f};')\n",
    "            if self.ckpt_dir:\n",
    "                torch.save(self.mlp.state_dict(), os.path.join(self.ckpt_dir, f'{epoch:03d}.pt'))\n",
    "    def load(self, x: np.ndarray, y: np.ndarray, ckpt_path: str):\n",
    "        self.define_mlp(x=x, y=y)\n",
    "        assert os.path.exists(ckpt_path)\n",
    "        self.mlp.load_state_dict(torch.load(ckpt_path))\n",
    "    def fit(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.define_mlp(x=x, y=y)\n",
    "        self.train_mlp(x=x, y=y)\n",
    "    def predict(self, x: np.ndarray):\n",
    "        self.mlp.eval()\n",
    "        x = torch.from_numpy(x).float()\n",
    "        dataset = TensorDataset(x)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, in dataloader:\n",
    "                outputs = self.mlp(inputs)\n",
    "                labels = outputs.argmax(dim=1)\n",
    "                y_pred.append(labels)\n",
    "        y_pred = torch.cat(y_pred, dim=0)\n",
    "        return y_pred.detach().cpu().numpy()\n",
    "\n",
    "classifier = MLPClassifier(ckpt_dir='./ckpt/')\n",
    "\n",
    "fix_seed(GLOBAL_SEED)\n",
    "_start_time = time.time()\n",
    "classifier.fit(train_x, train_y)\n",
    "test_y_hat = classifier.predict(test_x)\n",
    "accuracy = accuracy_score(test_y, test_y_hat)\n",
    "_end_time = time.time()\n",
    "_elapsed_time = _end_time - _start_time\n",
    "print(f'\\033[1m[{classifier.__class__.__name__}]\\033[0m elapsed={_elapsed_time:.2f}s; accuracy={accuracy*100:.2f}%;')\n",
    "\n",
    "del classifier, test_y_hat, accuracy\n",
    "del _start_time, _end_time, _elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制折线图可视化神经网络在训练集和测试集上的准确率随训练轮数变化\n",
    "\n",
    "classifier = MLPClassifier(ckpt_dir=None)\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "for epoch in range(16):\n",
    "    classifier.load(train_x, train_y, os.path.join('./ckpt/', f'{epoch:03d}.pt'))\n",
    "    train_y_hat = classifier.predict(train_x)\n",
    "    train_acc = accuracy_score(train_y, train_y_hat)\n",
    "    train_accs.append(train_acc)\n",
    "    test_y_hat = classifier.predict(test_x)\n",
    "    test_acc = accuracy_score(test_y, test_y_hat)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "# print(f'train_accs = {train_accs};')\n",
    "# print(f'test_accs = {test_accs};')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) [附加5pts] 基于 `mindspore` 训练神经网络模型\n",
    "\n",
    "使用国产化软件复现 (3) 的结果, 并比较二者在效率等方面的差异."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "\n",
    "class MLP(nn.Cell):\n",
    "    def __init__(self, input_dim: int, hidden_dims: list, output_dim: int):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_layer = nn.SequentialCell(\n",
    "            nn.Dense(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.hidden_layers = [\n",
    "            nn.SequentialCell(\n",
    "                nn.Dense(dim_in, dim_out),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            for dim_in, dim_out in zip(hidden_dims[:-1], hidden_dims[+1:])\n",
    "        ]\n",
    "        self.hidden_layers = nn.SequentialCell(*self.hidden_layers)\n",
    "        self.output_layer = nn.SequentialCell(\n",
    "            nn.Dense(hidden_dims[-1], output_dim),\n",
    "        )\n",
    "    def construct(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = hidden_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "class MLPClassifier(object):\n",
    "    def __init__(self, hidden_dims: list = [128, 32], batch_size: int = 128, learning_rate: float = 1e-2, num_epochs: int = 16, ckpt_dir: str = None):\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "    def define_mlp(self, x: np.ndarray, y: np.ndarray):\n",
    "        _, self.input_dim = x.shape\n",
    "        self.output_dim = y.max() + 1\n",
    "        self.input_dim = int(self.input_dim)\n",
    "        self.output_dim = int(self.output_dim)\n",
    "        self.mlp = MLP(input_dim=self.input_dim, hidden_dims=self.hidden_dims, output_dim=self.output_dim)\n",
    "    def train_mlp(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.mlp.set_train(True)\n",
    "        x = mindspore.Tensor.from_numpy(x).float()\n",
    "        y = mindspore.Tensor.from_numpy(y).int()\n",
    "        dataset = mindspore.dataset.GeneratorDataset([(xi, yi) for xi, yi in zip(x, y)], column_names=['inputs', 'labels'], shuffle=True).batch(batch_size=self.batch_size)\n",
    "        dataloader = dataset.create_tuple_iterator()\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = nn.Adam(self.mlp.trainable_params(), learning_rate=self.learning_rate)\n",
    "        def forward_fn(inputs, labels):\n",
    "            logits = self.mlp(inputs)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            return loss, logits\n",
    "        grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            loss = 0.0\n",
    "            for inputs, labels in dataloader:\n",
    "                raise NotImplementedError()\n",
    "                # TODO: 阅读官方文档示例, 完成梯度下降的代码, 注意把当前批次的损失加到 running_loss 上.\n",
    "                # https://www.mindspore.cn/tutorials/zh-CN/r2.3.0rc2/beginner/train.html#%E8%AE%AD%E7%BB%83%E4%B8%8E%E8%AF%84%E4%BC%B0\n",
    "            print(f'\\033[1m[{epoch+1:3d}/{self.num_epochs:3d}]\\033[0m loss={loss:8.4f};')\n",
    "            if self.ckpt_dir:\n",
    "                mindspore.save_checkpoint(self.mlp, os.path.join(self.ckpt_dir, f'{epoch:03d}.ckpt'))\n",
    "    def load(self, x: np.ndarray, y: np.ndarray, ckpt_path: str):\n",
    "        self.define_mlp(x=x, y=y)\n",
    "        assert os.path.exists(ckpt_path)\n",
    "        _, _ = mindspore.load_param_into_net(self.mlp, mindspore.load_checkpoint(ckpt_path))\n",
    "    def fit(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.define_mlp(x=x, y=y)\n",
    "        self.train_mlp(x=x, y=y)\n",
    "    def predict(self, x: np.ndarray):\n",
    "        self.mlp.set_train(False)\n",
    "        x = mindspore.Tensor.from_numpy(x).float()\n",
    "        dataset = mindspore.dataset.GeneratorDataset([xi for xi in x], column_names=['inputs'], shuffle=False).batch(batch_size=self.batch_size)\n",
    "        dataloader = dataset.create_tuple_iterator()\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, in dataloader:\n",
    "                outputs = self.mlp(inputs)\n",
    "                labels = outputs.argmax(axis=1)\n",
    "                y_pred.append(labels)\n",
    "        y_pred = ops.Concat(axis=0)(y_pred)\n",
    "        return y_pred.asnumpy()\n",
    "\n",
    "classifier = MLPClassifier(ckpt_dir='./ckpt/')\n",
    "\n",
    "fix_seed(GLOBAL_SEED)\n",
    "_start_time = time.time()\n",
    "classifier.fit(train_x, train_y)\n",
    "test_y_hat = classifier.predict(test_x)\n",
    "accuracy = accuracy_score(test_y, test_y_hat)\n",
    "_end_time = time.time()\n",
    "_elapsed_time = _end_time - _start_time\n",
    "print(f'\\033[1m[{classifier.__class__.__name__}]\\033[0m elapsed={_elapsed_time:.2f}s; accuracy={accuracy*100:.2f}%;')\n",
    "\n",
    "del classifier, test_y_hat, accuracy\n",
    "del _start_time, _end_time, _elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制折线图可视化神经网络在训练集和测试集上的准确率随训练轮数变化\n",
    "\n",
    "classifier = MLPClassifier(ckpt_dir=None)\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "for epoch in range(16):\n",
    "    classifier.load(train_x, train_y, os.path.join('./ckpt/', f'{epoch:03d}.ckpt'))\n",
    "    train_y_hat = classifier.predict(train_x)\n",
    "    train_acc = accuracy_score(train_y, train_y_hat)\n",
    "    train_accs.append(train_acc)\n",
    "    test_y_hat = classifier.predict(test_x)\n",
    "    test_acc = accuracy_score(test_y, test_y_hat)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "# print(f'train_accs = {train_accs};')\n",
    "# print(f'test_accs = {test_accs};')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 [15pts] 参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) [5pts] 5 折交叉验证\n",
    "\n",
    "调用 `sklearn` 实现, 为 $k$ 近邻选择最优的邻居数量 $k$, 汇报在训练集上选出的 $k$ 及其 5 折交叉验证准确率, $k \\in \\{1, \\cdots, 16\\}$. \n",
    "\n",
    "> - $k$=1, accuracy=????%;\n",
    "> - $k$=2, accuracy=????%;\n",
    "> - $k$=3, accuracy=????%;\n",
    "> - $k$=4, accuracy=????%;\n",
    "> - $k$=5, accuracy=????%;\n",
    "> - $k$=6, accuracy=????%;\n",
    "> - $k$=7, accuracy=????%;\n",
    "> - $k$=8, accuracy=????%;\n",
    "> - $k$=9, accuracy=????%;\n",
    "> - $k$=10, accuracy=????%;\n",
    "> - $k$=11, accuracy=????%;\n",
    "> - $k$=12, accuracy=????%;\n",
    "> - $k$=13, accuracy=????%;\n",
    "> - $k$=14, accuracy=????%;\n",
    "> - $k$=15, accuracy=????%;\n",
    "> - $k$=16, accuracy=????%;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from evaluate import evaluate_5_fold_cv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) [5pts] 多进程并行加速\n",
    "\n",
    "为高斯核支持向量机选择最优的正则化系数 $C$, 汇报在训练集上选出的 $C$ 及其5折交叉验证准确率, 同时汇报使用多进程并行加速后的总用时, $C \\in \\{0.01, 0.1, 1.0, 10.0, 100.0\\}$.\n",
    "\n",
    "> - $C$=0.01, accuracy=????%\n",
    "> - $C$=0.1, accuracy=????%\n",
    "> - $C$=1.0, accuracy=????%\n",
    "> - $C$=10.0, accuracy=????%\n",
    "> - $C$=100.0, accuracy=????%\n",
    "> - 总共用时 ????s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from evaluate import distributed_evaluate_classifier\n",
    "\n",
    "# 提示: 推荐使用 `joblib`, 其接口比 `multiprocessing` 更加简洁.\n",
    "# 注意: 如果你使用的操作系统是 Windows, 那么进程的入口函数必须从包文件导入.\n",
    "# 进程的入口函数已经实现, 即 distributed_evaluate_classifier, 你需要为其填充参数, 填充示例如下:\n",
    "# packed = [task_id, SVC, [], {'C': c, 'kernel': 'rbf'}, fit_x, fit_y, predict_x, predict_y]\n",
    "# 每个进程将执行如下代码:\n",
    "# distributed_evaluate_classifier(packed)\n",
    "# 其中, task_id 用来记录当前任务身份, 对于此处代码而言 task_id 应当包括 C 的值和交叉验证中的折数.\n",
    "# 其中, fit_x 和 fit_y 是这一折交叉验证的训练数据, predict_x 和 predict_y 是这一折交叉验证的测试数据.\n",
    "delayed_entrance = delayed(distributed_evaluate_classifier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) [5pts] 搜索超参数\n",
    "\n",
    "使用 `optuna` 搜索 `xgboost` 的超参数, 汇报在训练集上选出的超参数及其 5 折交叉验证准确率 (更换随机数种子不低于 93.0%).\n",
    "\n",
    "> - 关键超参数: n_estimators=????; max_depth=????; max_leaves=????; eta=????; 如果你搜索了其他超参数也一并填在这里\n",
    "> - 5 折交叉验证准确率: ????%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from evaluate import evaluate_5_fold_cv\n",
    "\n",
    "import optuna\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 [15pts] 集成模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) [5pts] 简单多数投票\n",
    "\n",
    "采用简单多数投票法集成之前题目调过参的分类器: $k$ 近邻, 高斯核支持向量机, `xgboost`.\n",
    "\n",
    "汇报测试集上准确率的提升.\n",
    "\n",
    "> 测试集上准确率的提升: ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) [5pts] Stacking\n",
    "\n",
    "采用 Stacking 集成上一问中的分类器, 通过 5 折交叉验证训练 Stacking 模型. 汇报测试集上准确率的提升.\n",
    "\n",
    "> 测试集上准确率的提升: ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) [5pts] 探索其他集成方式\n",
    "\n",
    "以下方式任选其一: 把神经网络最后一个隐层的输出作为新的特征, 训练一个根据样本决定采用哪个模型的路由模型, 提出你自己的集成方式并给出清晰的说明. 汇报测试集上准确率的提升.\n",
    "\n",
    "> 测试集上准确率的提升: ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 [附加5pts] 学件市场\n",
    "\n",
    "<h2>北冥坞的注册邮箱: <code>TODO@smail.nju.edu.cn</code></h2>\n",
    "\n",
    "<h2>上传的学件的 ID: <code>TODO</code></h2>\n",
    "\n",
    "<h2>根据规约查搜学件的截图:</h2>\n",
    "\n",
    "<h2>你认为北冥坞还需要改进的地方: (例如, 代码报错日志不够详细, 上传/复用学件存在明显的冗余操作步骤, &hellip;)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 致谢\n",
    "\n",
    "<h2>允许与其他同样未完成作业的同学讨论作业的内容, 但需在此注明并加以致谢; 如在作业过程中, 参考了互联网上的资料或大语言模型的生成结果, 且对完成作业有帮助的, 亦需注明并致谢.</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
