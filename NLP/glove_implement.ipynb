{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/wuzhen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 33.91818932490102\n",
      "Epoch 20/100, Loss: 16.218692874680112\n",
      "Epoch 30/100, Loss: 9.045507991413924\n",
      "Epoch 40/100, Loss: 5.60040106753086\n",
      "Epoch 50/100, Loss: 3.7466490316582846\n",
      "Epoch 60/100, Loss: 2.660240281601897\n",
      "Epoch 70/100, Loss: 1.9789659161694788\n",
      "Epoch 80/100, Loss: 1.5276067349629\n",
      "Epoch 90/100, Loss: 1.2147287371557507\n",
      "Epoch 100/100, Loss: 0.9895371014255213\n",
      "\n",
      "Word vector for 'language': [-0.66029827  0.95177254  0.74245279 -0.8051577   0.240141    0.35287231\n",
      "  0.88614436  0.14775036 -0.19508898 -1.1452313 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# 下载 punkt 分词工具\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 示例文本数据\n",
    "text_data = \"\"\"\n",
    "Natural language processing (NLP) is a field of artificial intelligence that deals with the interaction \n",
    "between computers and humans using natural language. The ultimate goal of NLP is to enable computers \n",
    "to understand, interpret, and generate human language in a way that is valuable.\n",
    "Word2Vec is a technique in NLP used to compute vector representations of words. These representations\n",
    "capture semantic meaning of words and are widely used for various NLP tasks such as word similarity, \n",
    "text classification, and machine translation.\n",
    "\"\"\"\n",
    "\n",
    "# 对文本进行分词\n",
    "sentences = nltk.sent_tokenize(text_data)\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# 构建词汇表\n",
    "vocab = set(word for sentence in tokenized_sentences for word in sentence)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 为每个词分配一个唯一的整数ID\n",
    "word2index = {word: i for i, word in enumerate(vocab)}\n",
    "index2word = {i: word for word, i in word2index.items()}\n",
    "\n",
    "# 计算词汇之间的共现矩阵\n",
    "window_size = 2  # 上下文窗口大小\n",
    "co_occurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# 构建共现矩阵\n",
    "for sentence in tokenized_sentences:\n",
    "    for i, word in enumerate(sentence):\n",
    "        target_idx = word2index[word]\n",
    "        for j in range(i - window_size, i + window_size + 1):\n",
    "            if j >= 0 and j < len(sentence) and i != j:\n",
    "                context_word = sentence[j]\n",
    "                context_idx = word2index[context_word]\n",
    "                co_occurrence_matrix[target_idx][context_idx] += 1\n",
    "\n",
    "# GloVe的训练参数\n",
    "embedding_dim = 10  # 词向量维度\n",
    "learning_rate = 0.05  # 学习率\n",
    "epochs = 100  # 训练轮次\n",
    "alpha = 0.75  # 加权函数的指数\n",
    "x_max = 100  # 加权函数的阈值\n",
    "\n",
    "# 初始化词向量\n",
    "W = np.random.randn(vocab_size, embedding_dim)  # 词向量矩阵\n",
    "W_prime = np.random.randn(vocab_size, embedding_dim)  # 上下文词向量矩阵\n",
    "b = np.zeros(vocab_size)  # 词偏置\n",
    "b_prime = np.zeros(vocab_size)  # 上下文词偏置\n",
    "\n",
    "# 加权函数\n",
    "def weighted_loss(x):\n",
    "    if x < x_max:\n",
    "        return (x / x_max) ** alpha\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# 损失函数\n",
    "def loss_fn(i, j, co_occurrence):\n",
    "    weight = weighted_loss(co_occurrence)\n",
    "    dot_product = np.dot(W[i], W_prime[j]) + b[i] + b_prime[j]\n",
    "    return weight * (dot_product - np.log(co_occurrence)) ** 2\n",
    "\n",
    "# 训练 GloVe 模型\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(vocab_size):\n",
    "        for j in co_occurrence_matrix[i]:\n",
    "            co_occurrence = co_occurrence_matrix[i][j]\n",
    "            # 计算损失函数\n",
    "            loss = loss_fn(i, j, co_occurrence)\n",
    "            total_loss += loss\n",
    "            \n",
    "            # 计算梯度\n",
    "            weight = weighted_loss(co_occurrence)\n",
    "            common_term = weight * (np.dot(W[i], W_prime[j]) + b[i] + b_prime[j] - np.log(co_occurrence))\n",
    "            \n",
    "            # 更新参数\n",
    "            W[i] -= learning_rate * common_term * W_prime[j]\n",
    "            W_prime[j] -= learning_rate * common_term * W[i]\n",
    "            b[i] -= learning_rate * common_term\n",
    "            b_prime[j] -= learning_rate * common_term\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# 预测词向量\n",
    "def predict_word(word):\n",
    "    word_idx = word2index[word]\n",
    "    return W[word_idx]\n",
    "\n",
    "# 使用训练好的模型进行预测\n",
    "print(\"\\nWord vector for 'language':\", predict_word('language'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python38)",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
