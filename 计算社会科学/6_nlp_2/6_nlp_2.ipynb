{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本向量化\n",
    "1. Gensim库简介\n",
    "2. 环境安装\n",
    "3. 词语编码\n",
    "4. 词向量模型训练\n",
    "5. 词语相似性计算\n",
    "6. 词向量模型可视化效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim简介\n",
    "Gensim是一个开源的Python工具包，专注于处理文本数据，将文档表示为语义向量（semantic vectors）。它广泛应用于自然语言处理（NLP）任务，尤其适用于大规模文本语料库的无监督学习。\n",
    "\n",
    "以下是一些Gensim库的功能和用法：\n",
    "\n",
    "1. **把文本变成向量**： 词袋模型（Bag-of-Words），TF-IDF（Term Frequency - Inverse Document Frequency）：\n",
    "2. **训练向量模型**：Word2Vec，Doc2Vec\n",
    "3. **计算文本相似度**：词与词之间的相似度，文档与文档之间的相似度\n",
    "4. **挖掘文档的内容主题**：从大量文档中自动识别出隐藏的“主题”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim核心概念\n",
    "#文档（Document）：一些文本。\n",
    "document = \"今天外卖点了爆辣螺狮粉，太好吃了\"\n",
    "#语料库 (Corpus)：文档的集合————用于模型训练的输入\n",
    "corpus = [\n",
    "    \"今天外卖点了爆辣螺狮粉，太好吃了\",\n",
    "    \"昨晚图书馆复习高数到凌晨一点，头都大了\",\n",
    "    \"舍友打游戏打到凌晨三点，我快崩溃了\",\n",
    "    \"体育课跑了八百米，我差点没缓过来\",\n",
    "    \"今天在食堂偶遇暗恋对象，心脏狂跳不止\",\n",
    "    \"打工兼职发传单，感觉自己像个社恐战士\",\n",
    "    \"被喜欢的人点赞朋友圈，开心到飞起\"\n",
    "]\n",
    "#向量：将每个文档表示为特征向量。\n",
    "#用数字来描述文档的“内容”\n",
    "#向量可以用于计算相似度、分类、聚类等任务\n",
    "vector_1=[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]\n",
    "vector_2=[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]\n",
    "\n",
    "#模型：将向量从一种表示形式转换为另一种表示形式的算法。可将新的词语句子进行编码/提取主题\n",
    "#gensim.models.TfidfModel \n",
    "#gensim.models.Word2Vec\n",
    "#gensim.models.Doc2Vec\n",
    "#gensim.models.LdaModel\n",
    "#gensim.models.FastText\n",
    "#gensim.models.KeyedVectors\n",
    "#gensim.models.LsiModel\n",
    "#gensim.models.Phrases\n",
    "#gensim.models.Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "import jieba\n",
    "\n",
    "corpus = [\n",
    "    \"今天外卖点了爆辣螺狮粉，太好吃了\",\n",
    "    \"昨晚图书馆复习高数到凌晨一点，头都大了\",\n",
    "    \"舍友打游戏打到凌晨三点，我快崩溃了\",\n",
    "    \"体育课跑了八百米，我差点没缓过来\",\n",
    "    \"今天在食堂偶遇暗恋对象，心脏狂跳不止\",\n",
    "    \"打工兼职发传单，感觉自己像个社恐战士\",\n",
    "    \"今天第一次尝试滑板，摔了个屁股墩\",\n",
    "    \"室友半夜吃泡面香死我了，忍不住也煮了一包\",\n",
    "    \"考前抱佛脚真的有用，我数学竟然及格了\",\n",
    "    \"被喜欢的人点赞朋友圈，开心到飞起\",\n",
    "    \"实验课做爆试管了，老师脸都黑了\"\n",
    "]\n",
    "\n",
    "def chinese_preprocess(text, stop_words):\n",
    "    words = jieba.lcut(text) # 分词\n",
    "    words = [word for word in words if word not in stop_words] # 去除停用词\n",
    "    words = [word for word in words if word.isalpha()] # 去除标点符号和数字\n",
    "    words = [token for token in words if all('\\u4e00' <= c <= '\\u9fa5' for c in token)] # 去除非中文\n",
    "    #words = [token for token in words if len(token) > 1] # 根据长度排除短词\n",
    "    return words\n",
    "\n",
    "stop_words = set(line.strip() for line in open('resource/中文stoplist.txt', 'r', encoding='utf-8'))\n",
    "tokenized_corpus = [chinese_preprocess(doc, stop_words) for doc in corpus]\n",
    "\n",
    "print(\"清洗后的分词结果：\")\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词袋编码\n",
    "from gensim import corpora\n",
    "\n",
    "#创建词典\n",
    "dictionary = corpora.Dictionary(tokenized_corpus) #每遇到一个新词，就分配一个唯一的整数 ID\n",
    "#print(dictionary.token2id) #“词到编号”的映射\n",
    "\n",
    "#将每个文档转换为词袋向量\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_corpus]\n",
    "for i, vec in enumerate(bow_corpus):\n",
    "    print(f\"文档{i+1}：{vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF编码\n",
    "from gensim.models import TfidfModel\n",
    "# 训练 TF-IDF 模型\n",
    "tfidf_model = TfidfModel(bow_corpus) # 使用词袋向量作为输入\n",
    "\n",
    "# 将原始的词袋向量转化为 TF-IDF 向量\n",
    "tfidf_corpus = tfidf_model[bow_corpus]\n",
    "\n",
    "print(\"\\nTF-IDF 向量（每个词的权重）：\")\n",
    "for i, vec in enumerate(tfidf_corpus):\n",
    "    print(f\"文档{i+1}：{vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec编码\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_corpus,  # 输入清洗好的语料\n",
    "    vector_size=256,     # 每个词向量的维度（推荐：100~300）\n",
    "    window=5,            # 上下文窗口大小（一个词左右各看几个词）\n",
    "    min_count=1,         # 忽略出现次数少于 min_count 的词（这里设为1以保留所有词）\n",
    "    workers=4,           # 并行训练的线程数（根据 CPU 核心数设置）\n",
    "    sg=1,                # 训练算法：0为CBOW，1为Skip-gram\n",
    "    epochs=50,            # 训练轮数（迭代整个语料的次数）\n",
    "    seed=123              # 随机种子，保证每次训练得到的向量相同\n",
    ")\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"word2vec_model.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入模型\n",
    "model = Word2Vec.load(\"word2vec_model.model\")\n",
    "print('词表长度：', len(model.wv))  #输出词向量表个数\n",
    "print('向量维度：',model.wv.vector_size) #词输出向量维度\n",
    "print('语料数：', model.corpus_count) #输出语料数\n",
    "\n",
    "# 找出与某个词最相似的前5个词\n",
    "analysis_word=\"今天\"\n",
    "similar_words = model.wv.most_similar(analysis_word, topn=5)\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: 相似度={similarity:.4f}\")\n",
    "\n",
    "# 计算两个词之间的余弦相似度\n",
    "similarity = model.wv.similarity(\"暗恋\", \"对象\")\n",
    "print(f\"'暗恋' 和 '对象' 的相似度：{similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载预训练的word2vec模型\n",
    "#https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "#下载预训练模型\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "##查看所有可能的模型\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "##模型命名<embedding_type>-<data_source>-<extra_info>-<dimensions>\n",
    "# 例如glove-wiki-gigaword-300\n",
    "# glove：词向量算法，GloVe（Global Vectors for Word Representation）\n",
    "# wiki-gigaword：训练语料来源，维基百科 + Gigaword（一个大型新闻语料库）\n",
    "# 300：词向量维度（embedding dimension）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('glove-wiki-gigaword-100') #加载训练好的模型并应用模型\n",
    "# 验证关键词语是否存在\n",
    "words_to_check = ['king', 'man', 'woman', 'queen']\n",
    "for word in words_to_check:\n",
    "    print(f\"'{word}' 存在于模型中: {word in model.key_to_index.keys()}\")\n",
    "\n",
    "# 执行：King - Man + Woman ≈ ?\n",
    "try:\n",
    "    result = model.most_similar(\n",
    "        positive=['king','woman'], # 正例\n",
    "        negative=['man'], # 负例\n",
    "        topn=10 \n",
    "    )\n",
    "    print(\"\\n结果:\")\n",
    "    for i,(word, similarity) in enumerate(result):\n",
    "        print(f\"Top{i+1}:{word} (相似度: {similarity})\")\n",
    "except KeyError as e:\n",
    "    print(f\"错误: 词语 {e} 不在词汇表中！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#首都的示例 paris - france + japan ？\n",
    "try:\n",
    "    result = model.most_similar(\n",
    "        positive=['paris','japan'], # 正例\n",
    "        negative=['france'], # 负例\n",
    "        topn=10 \n",
    "    )\n",
    "    print(\"\\n结果:\")\n",
    "    for i,(word, similarity) in enumerate(result):\n",
    "        print(f\"Top{i+1}:{word} (相似度: {similarity})\")\n",
    "except KeyError as e:\n",
    "    print(f\"错误: 词语 {e} 不在词汇表中！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dog - bark + meow ？ meow：喵喵叫\n",
    "try:\n",
    "    result = model.most_similar(\n",
    "        positive=['dog','meow'], # 正例 \n",
    "        negative=['bark'], # 负例\n",
    "        topn=10 \n",
    "    )\n",
    "    print(\"\\n结果:\")\n",
    "    for i,(word, similarity) in enumerate(result):\n",
    "        print(f\"Top{i+1}:{word} (相似度: {similarity})\")\n",
    "except KeyError as e:\n",
    "    print(f\"错误: 词语 {e} 不在词汇表中！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练自己word2vec模型以红楼梦为例。 林黛玉最相近的词汇有哪些？可视化红楼梦的词分布\n",
    "# 读取整个文本文件\n",
    "file_path = \"./resource/红楼梦\"\n",
    "import os\n",
    "#读取路径中所有的文件\n",
    "chapter_data = []\n",
    "for file_name in os.listdir(file_path):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        chapter_path = os.path.join(file_path, file_name)\n",
    "        with open(chapter_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            chapter_data.append((file_name, text))\n",
    "            print(file_name, text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##数据预处理，对清洗每一章节数据\n",
    "import jieba\n",
    "import re\n",
    "def wash_chinese(text):\n",
    "    washed_text=[token for token in jieba.cut_for_search(text)] #分词\n",
    "    stoplist=[line.strip() for line in open(r'resource/中文stoplist.txt').readlines()]\n",
    "    result=[token for token in washed_text if token not in stoplist]\n",
    "    result=[token for token in result if not token.isnumeric()] #排除数字\n",
    "    result=[token for token in result if all(map(lambda c:'\\u4e00' <= c <= '\\u9fa5',token))]  #排除非中文\n",
    "    result = [token for token in result if len(token)>1] #排除单字\n",
    "    return result\n",
    "##对每一章的内容进行数据预处理\n",
    "all_clead_data=[]\n",
    "for title, content in chapter_data:\n",
    "    cleaned_corpus = wash_chinese(content)\n",
    "    all_clead_data.append(cleaned_corpus)\n",
    "\n",
    "print(\"清洗前：\",chapter_data[0][1][:100])\n",
    "print(\"清洗后：\",all_clead_data[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "from gensim.models import word2vec\n",
    "train_data=all_clead_data\n",
    "model = word2vec.Word2Vec(sentences=train_data, sg=1, min_count=5,window=10 ,vector_size=300,epochs=50)\n",
    "hognlou_model_name=\"w2v_model_name_hongloumeng\"\n",
    "model.save(hognlou_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.vector_size) #词输出向量维度\n",
    "print('语料数：', model.corpus_count) #输出语料数\n",
    "print('词表长度：', len(model.wv.key_to_index))  \n",
    "print('词表内容',model.wv.key_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 查看跟林黛玉最近的100组词\n",
    "sim_result=model.wv.most_similar(positive=['黛玉'], topn=20)\n",
    "sim_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.wv.most_similar(\n",
    "    positive=['宝玉','姑娘'], # 正例\n",
    "    negative=['男子'], # 负例\n",
    "    topn=10 \n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可视化词嵌入\n",
    "#可视化方式1\n",
    "import csv\n",
    "from gensim.models import Word2Vec\n",
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#visualising-word-embeddings\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可视化方式1 matplotlib\n",
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "# 这段代码的主要作用：是对训练好的词向量模型进行降维处理，并将结果可视化。\n",
    "# 定义了一个函数 reduce_dimensions，用于将词向量从高维空间降到二维空间\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # 指定最终的维度为2D\n",
    "\n",
    "    # 提取模型中的词向量和对应的词汇列表\n",
    "    vectors = np.asarray(model.wv.vectors)  # 获取所有词向量\n",
    "    labels = np.asarray(model.wv.index_to_key)  # 获取所有词汇\n",
    "\n",
    "    # 使用 t-SNE 算法对词向量进行降维\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)  # 初始化 t-SNE 模型\n",
    "    vectors = tsne.fit_transform(vectors)  # 对词向量进行降维\n",
    "\n",
    "    # 分离降维后的坐标值\n",
    "    x_vals = [v[0] for v in vectors]  # 提取降维后每个词向量的第一个维度值\n",
    "    y_vals = [v[1] for v in vectors]  # 提取降维后每个词向量的第二个维度值\n",
    "    return x_vals, y_vals, labels  # 返回降维后的坐标值和词汇列表\n",
    "\n",
    "\n",
    "# 定义了一个函数 plot_with_matplotlib，用于绘制降维后的词向量分布图\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    # 设置字体为系统支持的中文字体（适用于 macOS）\n",
    "    plt.rcParams['font.family'] = 'Songti SC'\n",
    "\n",
    "    # 设置随机种子以保证结果可复现\n",
    "    random.seed(0)\n",
    "\n",
    "    # 创建一个 12x12 的绘图区域\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    # 绘制散点图\n",
    "    plt.scatter(x_vals, y_vals)  # 根据降维后的坐标值绘制散点图\n",
    "\n",
    "    # 随机选择部分词汇进行标注\n",
    "    indices = list(range(len(labels)))  # 获取所有词汇的索引\n",
    "    selected_indices = [i for i in range(10)]  # 选择前10个词汇进行标注\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))  # 在散点图上标注词汇\n",
    "\n",
    "# 加载之前训练好的 Word2Vec 模型\n",
    "hognlou_model = Word2Vec.load(hognlou_model_name)  \n",
    "\n",
    "# 对模型中的词向量进行降维处理\n",
    "x_vals, y_vals, labels = reduce_dimensions(hognlou_model)\n",
    "\n",
    "# 调用绘制函数，生成词向量分布图\n",
    "plot_with_matplotlib(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 可视化方式2  https://projector.tensorflow.org/\n",
    "import csv\n",
    "def w2v_visial(w2v_model_name,visial_name=\"\"):\n",
    "    model = Word2Vec.load(w2v_model_name) #加载并应用模型\n",
    "    vocabs = model.wv.key_to_index.keys()\n",
    "    print(vocabs)\n",
    "    for vocab in vocabs:\n",
    "        word=vocab\n",
    "        vector=model.wv[vocab]\n",
    "        with open(f'词向量分析/{visial_name}_w2v_vector.tsv','a',encoding='utf-8',newline='') as f_vec: #提取的特定信息文本\n",
    "            csv_writer=csv.writer(f_vec,delimiter='\\t')\n",
    "            csv_writer.writerow(vector)\n",
    "        with open(f'词向量分析/{visial_name}_w2v_word.tsv','a',encoding='utf-8') as f_word: #提取的特定信息文本\n",
    "            f_word.write(word.strip()+'\\n')\n",
    "#保存词表和对应的向量\n",
    "w2v_visial(hognlou_model_name,visial_name=\"honglou\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
